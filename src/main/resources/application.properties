spring.application.name=ragchat

vaadin.launch-browser=true
vaadin.allowed-packages=org.jbareaud.ragchat

rag-chat.content-retriever-max-results=10
rag-chat.memory-provider-max-messages=10

# Ollama
rag-chat.chat-ollama-base-url=http://localhost:11434
# Generally 0.0 is recommended, but 0.3 allows some stylistic leeway when displaying answers from a raw mediawiki-formatted knowledge base
rag-chat.chat-temperature=0.3
rag-chat.chat-top-k=5

# types : ONNX, LLM
rag-chat.scoring-type=ONNX
# ONNX reranker scoring model
rag-chat.onnx-scoring-path-to-model=D:\\LLM\\models\\ONNX\\mxbai-rerank-xsmall-v1\\model.onnx
rag-chat.onnx-scoring-path-to-tokenizer=D:\\LLM\\models\\ONNX\\mxbai-rerank-xsmall-v1\\tokenizer.json
# LLM reranker scoring model
rag-chat.llm-scoring-model-name=mistral-nemo:latest
# min score
rag-chat.scoring-min-score=0.6

# Chroma
rag-chat.chroma-enabled=true
rag-chat.chroma-base-url=http://localhost:8000
rag-chat.chroma-client-timeout=10s
rag-chat.chroma-store-timeout=60s

rag-chat.splitter-max-chars=500
rag-chat.splitter-overlap-chars=50

rag-chat.embedding-families=bert,nomic-bert
rag-chat.chat-families=llama,qwen2,qwen3,gemma3
rag-chat.default-chat-selection=qwen3:14b,mistral:latest,llama3.1:latest

# Debug
logging.level.dev.langchain4j=DEBUG
logging.level.org.jbareaud.ragchat=DEBUG
logging.level.com.vaadin.flow=WARN
logging.level.org.atmosphere.cpr=WARN
